{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "\n",
    "import openpifpaf  # pylint: disable=unused-import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Keypoints\n",
    "\n",
    " <div style=\"text-align: right\"> by <a href=\"https://scholar.google.com/citations?user=f-4YHeMAAAAJ&hl=en\">Lorenzo Bertoni </a> and Duncan Zauss, 13/04/2021 </div> <br /> \n",
    "\n",
    "\n",
    "\n",
    "This section describes the [OpenPifPaf](https://github.com/openpifpaf/openpifpaf) plugin for vehicles. The plugin uses the [ApolloCar3D Dataset](http://apolloscape.auto/car_instance.html). For more information, we suggest to check our latest [paper](https://arxiv.org/abs/2103.02440): <br /> \n",
    "\n",
    "> __OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association__<br />\n",
    "> _[Sven Kreiss](https://www.svenkreiss.com), [Lorenzo Bertoni](https://scholar.google.com/citations?user=f-4YHeMAAAAJ&hl=en), [Alexandre Alahi](https://scholar.google.com/citations?user=UIhXQ64AAAAJ&hl=en)_, 2021.\n",
    ">\n",
    "\n",
    "## Setup\n",
    "\n",
    "```sh\n",
    "pip3 install openpifpaf\n",
    "```\n",
    "\n",
    "(in case CUDA 9 as driver: \n",
    "` pip install torch==1.7.1+cu92 torchvision==0.8.2+cu92 -f https://download.pytorch.org/whl/torch_stable.html`)\n",
    "\n",
    "## Predict \n",
    "Prediction runs as standard openpifpaf predict command, but using the pretrained model on vehicles. The flag \"--checkpoint shufflenetv2k16-apollo-24\" will cause that our 24 kp version of the Shufflenet 16 (AP 76.1%) will be automatically downloaded. As an example, run the command:\n",
    "\n",
    "```sh\n",
    "python -m openpifpaf.predict \\\n",
    "<image path> \\\n",
    "--checkpoint shufflenetv2k16-apollo-24 -o \\\n",
    "--instance-threshold 0.07 --seed-threshold 0.07 \\\n",
    "--line-width 3 --font-size 0 --white-overlay 0.6 \n",
    "```\n",
    "\n",
    "## Preprocess Dataset\n",
    "### Keypoints choice\n",
    "The preprocessing step converts the annotations into the standard COCO format. It creates a version with all 66 keypoints and also creates a sparsified version with 24 keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpifpaf.plugins.apollocar3d.apollocar3d import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The car pose can be visualized with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_p90_x = np.array([[1, 0, 0], [0, 0, 1], [0, 1, 0], ])\n",
    "with openpifpaf.show.Canvas.blank(dpi=75, nomargin=True) as ax:\n",
    "    video_66 = constants.plot3d_red(ax, constants.CAR_POSE_66 @ rot_p90_x, constants.CAR_SKELETON_66).to_html5_video()\n",
    "IPython.display.HTML(video_66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with openpifpaf.show.Canvas.blank(dpi=75, nomargin=True) as ax:\n",
    "    video_24 = constants.plot3d_red(ax, constants.CAR_POSE_24 @ rot_p90_x, constants.CAR_SKELETON_24).to_html5_video()\n",
    "IPython.display.HTML(video_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset conversion\n",
    "To convert the original ApolloCar3D annotations into coco format: \n",
    "```sh\n",
    "mkdir data outputs apollo-coco, apollo-coco/images apollo-coco/annotations\n",
    "```\n",
    "\n",
    "then, download and soft link to [ApolloCar3D Dataset](http://apolloscape.auto/car_instance.html) into `data`\n",
    "    \n",
    "```sh\n",
    "pip install pandas\n",
    "pip install opencv-python==4.1.2.30\n",
    "```\n",
    "```sh\n",
    "python -m openpifpaf_apollocar3d.apollo_to_coco\n",
    "```\n",
    "\n",
    "This script will create annotations with 24kps and 66kps simultaneously. The argument `--split_images` copies the original images in the new folders according to the train val split, slowing down the process. No need to use it multiple times.\n",
    "\n",
    "\n",
    "## Train\n",
    "The default is training with 66kps\n",
    "Square-edge 769 (AP 76.1%)\n",
    "\n",
    "```sh\n",
    "python3 -m openpifpaf.train --dataset apollo \\\n",
    "--basenet=shufflenetv2k16 --apollo-square-edge=769 \\\n",
    "--lr=0.00002 --momentum=0.95  --b-scale=5.0 \\\n",
    "--epochs=300 --lr-decay 160 260 --lr-decay-epochs=10  --weight-decay=1e-5 \\\n",
    "--weight-decay=1e-5  --val-interval 10 --loader-workers 16 --apollo-upsample 2 \\\n",
    "--apollo-bmin 2 --batch-size 8\n",
    "```\n",
    "\n",
    "For smaller memory GPUs: square-edge 513\n",
    "\n",
    "```sh\n",
    "python3 -m openpifpaf.train --dataset apollo \\\n",
    "--basenet=shufflenetv2k16w --apollo-square-edge=513 \\\n",
    "--lr=0.00001 --momentum=0.98 --b-scale=20.0  --epochs=200 \\\n",
    "--lr-decay 130 140 --lr-decay-epochs=10  --weight-decay=1e-5  --loader-workers 16 \\\n",
    "  --val-interval 10 --batch-size 8 --apollo-upsample 2 --apollo-bmin 5\n",
    "```\n",
    "\n",
    "To train with 24kps you need to use the following command\n",
    "\n",
    "```sh\n",
    "python3 -m openpifpaf.train --dataset apollo \\\n",
    "--basenet=shufflenetv2k16 --apollo-square-edge=769 \\\n",
    "--lr=0.00002 --momentum=0.95  --b-scale=5.0 \\\n",
    "--epochs=300 --lr-decay 160 260 --lr-decay-epochs=10  --weight-decay=1e-5 \\\n",
    "--weight-decay=1e-5  --val-interval 10 --loader-workers 16 --apollo-upsample 2 \\\n",
    "--apollo-bmin 2 --batch-size 8 --apollo-use-24-kps --apollo-val-annotations \\\n",
    "<PathToThe/apollo_keypoints_24_train.json>\n",
    "```\n",
    "\n",
    "## Evaluation\n",
    "With 66 kps, replace shufflenetv2k16-apollo-66 with a path to your own checkpoint, if you want to evaluate on your own model:\n",
    "\n",
    "```sh\n",
    "CUDA_VISIBLE_DEVICES=0,1 python3 -m openpifpaf.eval --dataset=apollo \\\n",
    "--checkpoint shufflenetv2k16-apollo-66 \\\n",
    "--force-complete-pose --seed-threshold=0.01 --instance-threshold=0.01 \\\n",
    "--apollo-eval-long-edge 0\n",
    "```\n",
    "\n",
    "With 24 kps, replace shufflenetv2k16-apollo-24 with a path to your own checkpoint, if you want to evaluate on your own model. Note that also in evaluation flag you need to make sure to set the cli flag for using 24kps only:\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=0,1 python3 -m openpifpaf.eval --dataset=apollo \\\n",
    "--checkpoint <PathToYourCheckpoint> \\\n",
    "--force-complete-pose --seed-threshold=0.01 --instance-threshold=0.01 \\\n",
    "--apollo-eval-long-edge 0 --apollo-use-24-kps --apollo-val-annotations \\\n",
    "<PathToThe/apollo_keypoints_24_train.json>\n",
    "```\n",
    "\n",
    "## Everything else\n",
    "All pifpaf options and commands still stand, check them in the other sections of the guide.\n",
    "If you are interested in training your own dataset, read the section on a {doc}`custom dataset <plugins_custom>`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3('venv3')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "ea6946363a43e80d241452ab397f4c58bdd3d2517da174158e9c46ce6717422a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}