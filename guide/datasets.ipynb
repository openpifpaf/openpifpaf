{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "This section describes how to install common datasets used for training new \n",
    "models and for computing evaluation scores for entire datasets. In general, \n",
    "these datasets are large and require a computer with a good GPU to train and \n",
    "evaluate in reasonable times. Additional datasets are availble as plugins \n",
    "(for example {doc}`plugins_crowdpose`).\n",
    "\n",
    "```{note}\n",
    "These datasets are not required to do pose predictions on your own images.\n",
    "Even for training, you are unlikely to need all the datasets for your use case.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO data\n",
    "\n",
    "[COCO](http://cocodataset.org/) is a great datasets containing many types of annotations, including bounding boxes, 2D poses, etc.\n",
    "\n",
    "```sh\n",
    "mkdir data-mscoco\n",
    "cd data-mscoco\n",
    "\n",
    "wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n",
    "unzip annotations_trainval2017.zip\n",
    "unzip image_info_test2017.zip\n",
    "\n",
    "mkdir images\n",
    "cd images\n",
    "wget http://images.cocodataset.org/zips/val2017.zip\n",
    "wget http://images.cocodataset.org/zips/train2017.zip\n",
    "wget http://images.cocodataset.org/zips/test2017.zip\n",
    "unzip val2017.zip\n",
    "unzip train2017.zip\n",
    "unzip test2017.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO Person Skeletons\n",
    "\n",
    "COCO / kinematic tree / dense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpifpaf\n",
    "openpifpaf.plugin.register()\n",
    "%matplotlib inline\n",
    "openpifpaf.show.Canvas.show = True\n",
    "\n",
    "# first make an annotation\n",
    "ann_coco = openpifpaf.Annotation.from_cif_meta(\n",
    "    openpifpaf.plugins.coco.CocoKp().head_metas[0])\n",
    "ann_kin = openpifpaf.Annotation.from_cif_meta(\n",
    "    openpifpaf.plugins.coco.CocoKp(skeleton=openpifpaf.plugins.coco.constants.KINEMATIC_TREE_SKELETON).head_metas[0])\n",
    "ann_dense = openpifpaf.Annotation.from_cif_meta(\n",
    "    openpifpaf.plugins.coco.CocoKp(skeleton=openpifpaf.plugins.coco.constants.DENSER_COCO_PERSON_SKELETON).head_metas[0])\n",
    "\n",
    "# visualize the annotation\n",
    "openpifpaf.show.KeypointPainter.show_joint_scales = True\n",
    "keypoint_painter = openpifpaf.show.KeypointPainter()\n",
    "with openpifpaf.show.Canvas.annotation(ann_coco, ncols=3) as (ax1, ax2, ax3):\n",
    "    keypoint_painter.annotation(ax1, ann_coco)\n",
    "    keypoint_painter.annotation(ax2, ann_kin)\n",
    "    keypoint_painter.annotation(ax3, ann_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(coco-person-keypoints)=\n",
    "## COCO Person Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(openpifpaf.plugins.coco.constants.COCO_KEYPOINTS):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('associations')\n",
    "kp_names = openpifpaf.plugins.coco.constants.COCO_KEYPOINTS\n",
    "for i, (joint1, joint2) in enumerate(openpifpaf.plugins.coco.constants.COCO_PERSON_SKELETON):\n",
    "    print('{:2d}: {:15s} --> {}'.format(i, kp_names[joint1 - 1], kp_names[joint2 - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MPII data\n",
    "\n",
    "This MPII data is currently not used anywhere.\n",
    "\n",
    "```sh\n",
    "mkdir data-mpii\n",
    "cd data-mpii\n",
    "wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz\n",
    "wget https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NightOwls data\n",
    "\n",
    "```sh\n",
    "mkdir data-nightowls\n",
    "cd data-nightowls\n",
    "wget http://www.robots.ox.ac.uk/\\~vgg/data/nightowls/python/nightowls_validation.json\n",
    "wget http://www.robots.ox.ac.uk/\\~vgg/data/nightowls/python/nightowls_validation.zip\n",
    "unzip nightowls_validation.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv3': venv)",
   "metadata": {
    "interpreter": {
     "hash": "ea6946363a43e80d241452ab397f4c58bdd3d2517da174158e9c46ce6717422a"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
