{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sven Kreiss](https://www.svenkreiss.com/), 2020\n",
    "\n",
    "# Prediction CLI\n",
    "\n",
    "Use OpenPifPaf from the command line to run multi-person pose estimation on images.\n",
    "Below is also a short intro to running prediction on videos.\n",
    "\n",
    "Run `openpifpaf.predict` on an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DEBUG:__main__:neural network device: cpu\nDEBUG:openpifpaf.network.heads:cif = [0], caf = [1]\nDEBUG:openpifpaf.visualizer.base:('cif', 'cifdet'): indices = []\nDEBUG:openpifpaf.visualizer.base:seeds: indices = []\nDEBUG:openpifpaf.visualizer.base:occupancy: indices = []\nDEBUG:openpifpaf.visualizer.base:occupancy: indices = []\nDEBUG:openpifpaf.decoder.factory:head names = ('cif', 'caf', 'caf25')\nDEBUG:openpifpaf.visualizer.base:cif: indices = []\nDEBUG:openpifpaf.show.painters:color connections = False, lw = 2, marker = 6\nDEBUG:openpifpaf.visualizer.base:caf: indices = []\nDEBUG:openpifpaf.show.painters:color connections = False, lw = 2, marker = 6\nDEBUG:openpifpaf.show.painters:color connections = True, lw = 6, marker = 3\nDEBUG:openpifpaf.transforms.pad:valid area before pad: [  0.   0. 639. 426.], image size = (640, 427)\nDEBUG:openpifpaf.transforms.pad:pad with (0, 3, 1, 3)\nDEBUG:openpifpaf.transforms.pad:valid area after pad: [  0.   3. 639. 426.], image size = (641, 433)\nDEBUG:openpifpaf.decoder.generator.generator:nn processing time: 1.390s\nDEBUG:openpifpaf.decoder.generator.generator:parallel execution with worker <openpifpaf.decoder.generator.generator.DummyPool object at 0x12644f990>\nDEBUG:openpifpaf.decoder.generator.cifcaf:initial annotations = 0\nDEBUG:openpifpaf.decoder.cif_hr:target_intensities 0.004s\nDEBUG:openpifpaf.decoder.cif_seeds:seeds 771, 0.001s (C++ 0.000s)\nDEBUG:openpifpaf.decoder.caf_scored:scored caf (1339, 1296) in 0.003s\nDEBUG:openpifpaf.decoder.occupancy:shape = (17, 216, 320), min_scale = 2\nDEBUG:openpifpaf.decoder.generator.cifcaf:annotations 5, 0.038s\nDEBUG:openpifpaf.decoder.occupancy:shape = (17, 194, 283), min_scale = 2\nDEBUG:openpifpaf.decoder.nms:nms = 0.003\nINFO:openpifpaf.decoder.generator.cifcaf:5 annotations: [14, 16, 17, 16, 15]\nDEBUG:openpifpaf.decoder.generator.generator:time: nn = 1.391s, dec = 0.046s\nINFO:__main__:batch 0: coco/000000081988.jpg\nDEBUG:__main__:json output = coco/000000081988.jpg.predictions.json\nDEBUG:__main__:image output = coco/000000081988.jpg.predictions.png\n"
    }
   ],
   "source": [
    "!python -m openpifpaf.predict coco/000000081988.jpg --image-output --json-output --debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command produced two outputs: an image and a json file.\n",
    "You can provide file or folder arguments to the `--image-output` and `--json-output` flags.\n",
    "Here, we used the default which created these two files:\n",
    "\n",
    "```sh\n",
    "coco/000000081988.jpg.predictions.png\n",
    "coco/000000081988.jpg.predictions.json\n",
    "```\n",
    "\n",
    "Here is the image:\n",
    "\n",
    "![predictions](coco/000000081988.jpg.predictions.png)\n",
    "\n",
    "And here is the json output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\n    {\n        \"keypoints\": [\n            0.0,\n            -3.0,\n            0.0,\n            0.0,\n            -3.0,\n            0.0,\n            0.0,\n            -3.0,\n            0.0,\n            388.6,\n            149.4,\n            0.87,\n            410.08,\n            149.7,\n            0.73,\n            376.76,\n            176.22,\n            0.87,\n            425.65,\n            177.54,\n            0.87,\n            335.6,\n            192.22,\n            0.86,\n            463.58,\n            191.29,\n            0.85,\n            300.04,\n            194.49,\n            0.88,\n            494.45,\n            183.96,\n            0.87,\n            387.99,\n            254.67,\n            0.84,\n            414.94,\n            254.72,\n            0.92,\n            382.17,\n            315.62,\n            0.7,\n            410.01,\n            318.07,\n            1.0,\n            381.87,\n            362.75,\n            0.29,\n            405.22,\n            369.85,\n            0.76\n        ],\n        \"bbox\": [\n            289.18,\n            142.4,\n            216.91,\n            243.23\n        ],\n        \"score\": 0.735,\n        \"category_id\": 1\n    },\n    {\n        \"keypoints\": [\n            81.78,\n            317.29,\n            0.6,\n            85.3,\n            313.22,\n            0.77,\n            80.19,\n            312.1,\n            0.33,\n            99.85,\n            309.14,\n            0.78,\n            0.0,\n            -3.0,\n            0.0,\n            123.39,\n            317.32,\n            0.86,\n            78.56,\n            320.77,\n            0.77,\n            146.09,\n            348.52,\n            0.78,\n            58.28,\n            350.78,\n            0.77,\n            124.86,\n            354.53,\n            0.69,\n            52.18,\n            382.47,\n            0.77,\n            121.57,\n            363.04,\n            0.72,\n            94.3,\n            363.63,\n            0.73,\n            153.08,\n            362.21,\n            0.63,\n            73.61,\n            366.02,\n            0.68,\n            94.22,\n            380.86,\n            0.54,\n            116.73,\n            381.85,\n            0.42\n        ],\n        \"bbox\": [\n            46.49,\n            306.15,\n            114.13,\n            83.72\n        ],\n        \"score\": 0.682,\n        \"category_id\": 1\n    },\n    {\n        \"keypoints\": [\n            359.89,\n            299.08,\n            0.56,\n            363.57,\n            294.22,\n            0.59,\n            354.82,\n            294.77,\n            0.58,\n            370.09,\n            296.68,\n            0.55,\n            347.56,\n            297.82,\n            0.78,\n            380.37,\n            318.59,\n            0.72,\n            340.45,\n            322.22,\n            0.84,\n            384.69,\n            342.85,\n            0.6,\n            334.77,\n            347.95,\n            0.75,\n            372.99,\n            357.23,\n            0.52,\n            335.12,\n            362.72,\n            0.74,\n            375.05,\n            363.53,\n            0.67,\n            350.59,\n            363.54,\n            0.76,\n            388.25,\n            361.11,\n            0.34,\n            330.83,\n            371.03,\n            0.55,\n            334.89,\n            381.55,\n            0.31,\n            334.55,\n            384.24,\n            0.27\n        ],\n        \"bbox\": [\n            324.2,\n            292.08,\n            70.77,\n            98.82\n        ],\n        \"score\": 0.647,\n        \"category_id\": 1\n    },\n    {\n        \"keypoints\": [\n            493.2,\n            348.52,\n            0.25,\n            495.89,\n            345.12,\n            0.32,\n            489.55,\n            345.98,\n            0.2,\n            504.53,\n            334.61,\n            0.39,\n            0.0,\n            -3.0,\n            0.0,\n            522.68,\n            332.36,\n            0.57,\n            490.49,\n            333.07,\n            0.41,\n            549.31,\n            351.24,\n            0.49,\n            488.25,\n            352.99,\n            0.35,\n            561.04,\n            380.62,\n            0.31,\n            488.01,\n            376.64,\n            0.46,\n            540.84,\n            344.21,\n            0.57,\n            520.91,\n            346.19,\n            0.44,\n            521.27,\n            377.62,\n            0.75,\n            501.32,\n            379.69,\n            0.47,\n            565.13,\n            385.18,\n            0.43,\n            546.0,\n            383.56,\n            0.18\n        ],\n        \"bbox\": [\n            483.62,\n            327.0,\n            87.72,\n            64.39\n        ],\n        \"score\": 0.45,\n        \"category_id\": 1\n    },\n    {\n        \"keypoints\": [\n            237.64,\n            320.42,\n            0.25,\n            238.84,\n            316.65,\n            0.16,\n            234.91,\n            318.16,\n            0.29,\n            0.0,\n            -3.0,\n            0.0,\n            222.43,\n            315.36,\n            0.5,\n            240.26,\n            318.55,\n            0.71,\n            202.03,\n            319.22,\n            0.59,\n            240.3,\n            350.9,\n            0.6,\n            195.1,\n            352.77,\n            0.36,\n            240.05,\n            380.3,\n            0.48,\n            193.3,\n            376.3,\n            0.34,\n            221.76,\n            332.86,\n            0.56,\n            200.01,\n            331.03,\n            0.47,\n            223.93,\n            368.52,\n            0.36,\n            195.41,\n            371.26,\n            0.33,\n            0.0,\n            -3.0,\n            0.0,\n            194.78,\n            378.61,\n            0.21\n        ],\n        \"bbox\": [\n            189.05,\n            313.07,\n            56.25,\n            71.28\n        ],\n        \"score\": 0.435,\n        \"category_id\": 1\n    }\n]\n"
    }
   ],
   "source": [
    "!python -m json.tool coco/000000081988.jpg.predictions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Arguments\n",
    "\n",
    "* `--show`: show interactive matplotlib output\n",
    "* `--debug-images`: enable debug messages and debug plots\n",
    "\n",
    "Full list of arguments is available with `--help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "usage: python3 -m openpifpaf.predict [-h] [--version]\n                                     [--checkpoint CHECKPOINT]\n                                     [--basenet BASENET]\n                                     [--headnets HEADNETS [HEADNETS ...]]\n                                     [--no-pretrain] [--two-scale]\n                                     [--multi-scale] [--no-multi-scale-hflip]\n                                     [--cross-talk CROSS_TALK]\n                                     [--head-dropout HEAD_DROPOUT]\n                                     [--head-quad HEAD_QUAD]\n                                     [--seed-threshold SEED_THRESHOLD]\n                                     [--instance-threshold INSTANCE_THRESHOLD]\n                                     [--keypoint-threshold KEYPOINT_THRESHOLD]\n                                     [--decoder-workers DECODER_WORKERS]\n                                     [--dense-connections]\n                                     [--dense-coupling DENSE_COUPLING]\n                                     [--caf-seeds] [--force-complete-pose]\n                                     [--profile-decoder PROFILE_DECODER]\n                                     [--cif-th CIF_TH] [--caf-th CAF_TH]\n                                     [--connection-method {max,blend}]\n                                     [--greedy] [--show-box]\n                                     [--show-joint-scales]\n                                     [--show-joint-confidences]\n                                     [--show-decoding-order]\n                                     [--show-frontier-order]\n                                     [--show-only-decoded-connections]\n                                     [--debug-cifhr] [--debug-cif-c]\n                                     [--debug-cif-v] [--debug-cifdet-c]\n                                     [--debug-cifdet-v] [--debug-caf-c]\n                                     [--debug-caf-v]\n                                     [--debug-indices DEBUG_INDICES [DEBUG_INDICES ...]]\n                                     [--glob GLOB] [--show]\n                                     [--image-output [IMAGE_OUTPUT [IMAGE_OUTPUT ...]]]\n                                     [--json-output [JSON_OUTPUT [JSON_OUTPUT ...]]]\n                                     [--batch-size BATCH_SIZE]\n                                     [--long-edge LONG_EDGE]\n                                     [--loader-workers LOADER_WORKERS]\n                                     [--disable-cuda]\n                                     [--line-width LINE_WIDTH]\n                                     [--monocolor-connections]\n                                     [--figure-width FIGURE_WIDTH]\n                                     [--dpi-factor DPI_FACTOR] [-q] [--debug]\n                                     [images [images ...]]\n\nPredict poses for given images.\n\npositional arguments:\n  images                input images (default: None)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  --glob GLOB           glob expression for input images (for many images)\n                        (default: None)\n  --show                show image of output overlay (default: False)\n  --image-output [IMAGE_OUTPUT [IMAGE_OUTPUT ...]]\n                        image output file or directory (default: None)\n  --json-output [JSON_OUTPUT [JSON_OUTPUT ...]]\n                        json output file or directory (default: None)\n  --batch-size BATCH_SIZE\n                        processing batch size (default: 1)\n  --long-edge LONG_EDGE\n                        apply preprocessing to batch images (default: None)\n  --loader-workers LOADER_WORKERS\n                        number of workers for data loading (default: None)\n  --disable-cuda        disable CUDA (default: False)\n  --line-width LINE_WIDTH\n                        line width for skeleton (default: 6)\n  --monocolor-connections\n  --figure-width FIGURE_WIDTH\n                        figure width (default: 10.0)\n  --dpi-factor DPI_FACTOR\n                        increase dpi of output image by this factor (default:\n                        1.0)\n\nnetwork configuration:\n  --checkpoint CHECKPOINT\n                        Load a model from a checkpoint. Use \"resnet50\",\n                        \"resnet101\" or \"resnet152\" for pretrained OpenPifPaf\n                        models. (default: None)\n  --basenet BASENET     base network, e.g. resnet50 (default: None)\n  --headnets HEADNETS [HEADNETS ...]\n                        head networks (default: None)\n  --no-pretrain         create model without ImageNet pretraining (default:\n                        True)\n  --two-scale           [experimental] (default: False)\n  --multi-scale         [experimental] (default: False)\n  --no-multi-scale-hflip\n                        [experimental] (default: True)\n  --cross-talk CROSS_TALK\n                        [experimental] (default: 0.0)\n\nhead:\n  --head-dropout HEAD_DROPOUT\n                        [experimental] zeroing probability of feature in head\n                        input (default: 0.0)\n  --head-quad HEAD_QUAD\n                        number of times to apply quad (subpixel conv) to heads\n                        (default: 1)\n\ndecoder configuration:\n  --seed-threshold SEED_THRESHOLD\n                        minimum threshold for seeds (default: 0.5)\n  --instance-threshold INSTANCE_THRESHOLD\n                        filter instances by score (default: 0.1)\n  --keypoint-threshold KEYPOINT_THRESHOLD\n                        filter keypoints by score (default: None)\n  --decoder-workers DECODER_WORKERS\n                        number of workers for pose decoding (default: None)\n  --dense-connections   use dense connections (default: False)\n  --dense-coupling DENSE_COUPLING\n                        dense coupling (default: 0.01)\n  --caf-seeds           [experimental] (default: False)\n  --force-complete-pose\n  --profile-decoder PROFILE_DECODER\n                        specify out .prof file or empty string (default: None)\n\nCifCaf decoders:\n  --cif-th CIF_TH       cif threshold (default: 0.1)\n  --caf-th CAF_TH       caf threshold (default: 0.1)\n  --connection-method {max,blend}\n                        connection method to use, max is faster (default:\n                        blend)\n  --greedy              greedy decoding (default: False)\n\nshow:\n  --show-box\n  --show-joint-scales\n  --show-joint-confidences\n  --show-decoding-order\n  --show-frontier-order\n  --show-only-decoded-connections\n\npose visualizer:\n  --debug-cifhr\n  --debug-cif-c\n  --debug-cif-v\n  --debug-cifdet-c\n  --debug-cifdet-v\n  --debug-caf-c\n  --debug-caf-v\n  --debug-indices DEBUG_INDICES [DEBUG_INDICES ...]\n                        indices of fields to create debug plots for of the\n                        form headname:fieldindex, e.g. cif:5 (default: [])\n\nlogging:\n  -q, --quiet           only show warning messages or above (default: False)\n  --debug               print debug messages (default: False)\n"
    }
   ],
   "source": [
    "!python -m openpifpaf.predict --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video\n",
    "\n",
    "Requires OpenCV. The `--video-output` option also requires matplotlib.\n",
    "\n",
    "```sh\n",
    "python3 -m openpifpaf.video --source myvideotoprocess.mp4 --video-output --json-output\n",
    "```\n",
    "\n",
    "Replace `myvideotoprocess.mp4` with `0` for webcam0 or other OpenCV compatible sources.\n",
    "\n",
    "All possible arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "usage: python3 -m openpifpaf.video [-h] [--version] [--checkpoint CHECKPOINT]\n                                   [--basenet BASENET]\n                                   [--headnets HEADNETS [HEADNETS ...]]\n                                   [--no-pretrain] [--two-scale]\n                                   [--multi-scale] [--no-multi-scale-hflip]\n                                   [--cross-talk CROSS_TALK]\n                                   [--head-dropout HEAD_DROPOUT]\n                                   [--head-quad HEAD_QUAD]\n                                   [--seed-threshold SEED_THRESHOLD]\n                                   [--instance-threshold INSTANCE_THRESHOLD]\n                                   [--keypoint-threshold KEYPOINT_THRESHOLD]\n                                   [--decoder-workers DECODER_WORKERS]\n                                   [--dense-connections]\n                                   [--dense-coupling DENSE_COUPLING]\n                                   [--caf-seeds] [--force-complete-pose]\n                                   [--profile-decoder PROFILE_DECODER]\n                                   [--cif-th CIF_TH] [--caf-th CAF_TH]\n                                   [--connection-method {max,blend}]\n                                   [--greedy] [--show-box]\n                                   [--show-joint-scales]\n                                   [--show-joint-confidences]\n                                   [--show-decoding-order]\n                                   [--show-frontier-order]\n                                   [--show-only-decoded-connections]\n                                   [--debug-cifhr] [--debug-cif-c]\n                                   [--debug-cif-v] [--debug-cifdet-c]\n                                   [--debug-cifdet-v] [--debug-caf-c]\n                                   [--debug-caf-v]\n                                   [--debug-indices DEBUG_INDICES [DEBUG_INDICES ...]]\n                                   [--source SOURCE]\n                                   [--video-output [VIDEO_OUTPUT [VIDEO_OUTPUT ...]]]\n                                   [--video-fps VIDEO_FPS] [--show]\n                                   [--horizontal-flip]\n                                   [--no-colored-connections] [--disable-cuda]\n                                   [--scale SCALE] [--start-frame START_FRAME]\n                                   [--skip-frames SKIP_FRAMES]\n                                   [--max-frames MAX_FRAMES]\n                                   [--json-output [JSON_OUTPUT [JSON_OUTPUT ...]]]\n                                   [-q] [--debug]\n\nVideo demo application.\n\nUse --scale=0.2 to reduce the input image size to 20%.\nUse --json-output for headless processing.\n\nExample commands:\n    python3 -m pifpaf.video --source=0  # default webcam\n    python3 -m pifpaf.video --source=1  # another webcam\n\n    # streaming source\n    python3 -m pifpaf.video --source=http://127.0.0.1:8080/video\n\n\n    python3 -m pifpaf.video --source=docs/coco/000000081988.jpg\n\nTrouble shooting:\n* MacOSX: try to prefix the command with \"MPLBACKEND=MACOSX\".\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program's version number and exit\n  --source SOURCE       OpenCV source url. Integer for webcams. Supports rtmp\n                        streams. (default: None)\n  --video-output [VIDEO_OUTPUT [VIDEO_OUTPUT ...]]\n                        video output file (default: None)\n  --video-fps VIDEO_FPS\n  --show\n  --horizontal-flip\n  --no-colored-connections\n                        do not use colored connections to draw poses (default:\n                        True)\n  --disable-cuda        disable CUDA (default: False)\n  --scale SCALE         input image scale factor (default: 1.0)\n  --start-frame START_FRAME\n  --skip-frames SKIP_FRAMES\n  --max-frames MAX_FRAMES\n  --json-output [JSON_OUTPUT [JSON_OUTPUT ...]]\n                        json output file (default: None)\n\nnetwork configuration:\n  --checkpoint CHECKPOINT\n                        Load a model from a checkpoint. Use \"resnet50\",\n                        \"resnet101\" or \"resnet152\" for pretrained OpenPifPaf\n                        models. (default: None)\n  --basenet BASENET     base network, e.g. resnet50 (default: None)\n  --headnets HEADNETS [HEADNETS ...]\n                        head networks (default: None)\n  --no-pretrain         create model without ImageNet pretraining (default:\n                        True)\n  --two-scale           [experimental] (default: False)\n  --multi-scale         [experimental] (default: False)\n  --no-multi-scale-hflip\n                        [experimental] (default: True)\n  --cross-talk CROSS_TALK\n                        [experimental] (default: 0.0)\n\nhead:\n  --head-dropout HEAD_DROPOUT\n                        [experimental] zeroing probability of feature in head\n                        input (default: 0.0)\n  --head-quad HEAD_QUAD\n                        number of times to apply quad (subpixel conv) to heads\n                        (default: 1)\n\ndecoder configuration:\n  --seed-threshold SEED_THRESHOLD\n                        minimum threshold for seeds (default: 0.5)\n  --instance-threshold INSTANCE_THRESHOLD\n                        filter instances by score (default: 0.1)\n  --keypoint-threshold KEYPOINT_THRESHOLD\n                        filter keypoints by score (default: None)\n  --decoder-workers DECODER_WORKERS\n                        number of workers for pose decoding (default: None)\n  --dense-connections   use dense connections (default: False)\n  --dense-coupling DENSE_COUPLING\n                        dense coupling (default: 0.01)\n  --caf-seeds           [experimental] (default: False)\n  --force-complete-pose\n  --profile-decoder PROFILE_DECODER\n                        specify out .prof file or empty string (default: None)\n\nCifCaf decoders:\n  --cif-th CIF_TH       cif threshold (default: 0.1)\n  --caf-th CAF_TH       caf threshold (default: 0.1)\n  --connection-method {max,blend}\n                        connection method to use, max is faster (default:\n                        blend)\n  --greedy              greedy decoding (default: False)\n\nshow:\n  --show-box\n  --show-joint-scales\n  --show-joint-confidences\n  --show-decoding-order\n  --show-frontier-order\n  --show-only-decoded-connections\n\npose visualizer:\n  --debug-cifhr\n  --debug-cif-c\n  --debug-cif-v\n  --debug-cifdet-c\n  --debug-cifdet-v\n  --debug-caf-c\n  --debug-caf-v\n  --debug-indices DEBUG_INDICES [DEBUG_INDICES ...]\n                        indices of fields to create debug plots for of the\n                        form headname:fieldindex, e.g. cif:5 (default: [])\n\nlogging:\n  -q, --quiet           only show warning messages or above (default: False)\n  --debug               print debug messages (default: False)\n"
    }
   ],
   "source": [
    "!python -m openpifpaf.video --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image credit: \"[Learning to surf](https://www.flickr.com/photos/fotologic/6038911779/in/photostream/)\" by fotologic which is licensed under [CC-BY-2.0].\n",
    "\n",
    "[CC-BY-2.0]: https://creativecommons.org/licenses/by/2.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitvenv3venve864f70b47a24f709eace0523e013bb0",
   "display_name": "Python 3.7.4 64-bit ('venv3': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}